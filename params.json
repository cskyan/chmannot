{
  "name": "chmannot",
  "tagline": "Cancer Hallmark Annotator",
  "body": "# Cancer Hallmark Annotator\r\n\r\n`chmannot` is an automatic annotator of cancer hallmark on biomedical literature. It supports abstract-level annotation which means that given the abstract of a paper in PubMed it could predict the hallmark labels related to this paper. It is mainly used to evaluate the computational models and tune the model parameters. It also provides several utility functions to manipulate the dataset and post-process the results.\r\n\r\n## Getting Started\r\n\r\nThe following instructions will help you get a copy of the source code as well as the datasets, and run the programs on your own machine.\r\n\r\n### Prerequisities\r\n\r\nFirstly, you need to install a Python Interpreter (tested 2.7.12) and these packages:\r\n\r\n* numpy (tested 1.11.1)\r\n* scipy (tested 1.11.1)\r\n* matplotlib (tested 1.5.1)\r\n* pandas (tested 0.18.1)\r\n* scikit-learn (tested 0.17.1)\r\n* pyyaml (test 3.11)\r\n* openpyxl (test 2.3.2)\r\n* rdflib \\[optional\\] \\(tested 4.2.1\\)\r\n\r\nThe simplest way to get started is to use [Anaconda](https://www.continuum.io/anaconda-overview) Python distribution. If you have limited disk space, the [Miniconda](http://conda.pydata.org/miniconda.html) installer is recommended. After installing Miniconda and adding the path of folder `bin` to `$PATH` variable, run the following command:\r\n\r\n```bash\r\nconda install scikit-learn pandas matplotlib openpyxl\r\n```\r\n\r\n### Download the Source Code\r\n\r\nYou can clone the repository of this project and then update the submodule after entering the main folder:\r\n\r\n```bash\r\ngit clone https://github.com/cskyan/chmannot.git\r\ncd hmannot\r\ngit submodule update --init --recursive\r\n```\r\n\r\nOr you can clone the repository and submodules simultaneously:\r\n\r\n```bash\r\ngit clone --recursive https://github.com/cskyan/chmannot.git\r\n```\r\n\r\n### Configure Environment Variable\r\n\r\n* Add the path of folder `bin` to `$PATH` variable so that you can run the scripts wherever you want. *Remember to grant execution permissions to all the files located in* `bin`\r\n* Add the path of folder `lib` to `$PYTHONPATH` variable so that the Python Interpreter can find the library `bionlp`.\r\n\r\n### Configuration File\r\n\r\nThe global configuration file is stored as `etc/config.yaml`. The configurations of different functions in different modules are separated, which looks like the code snippet below.\r\n\r\n```\r\nMODULE1:\r\n- function: FUNCTION1\r\n  params:\r\n    PARAMETER1: VALUE1\r\n    PARAMETER2: VALUE2\r\n- function: FUNCTION2\r\n  params:\r\n    PARAMETER1: VALUE1\r\n\t\r\nMODULE2:\r\n- function: FUNCTION1\r\n  params:\r\n    PARAMETER1: VALUE1\r\n```\r\n\r\nHence you can access a specific parameter VALUE using a triple (MODULE, FUNCTION, PARAMETER). The utility function `cfg_reader` in `bionlp.util.io` can be used to read the parameters in the configuration file:\r\n\r\n```python\r\nimport bionlp.util.io as io\r\ncfgr = io.cfg_reader(CONFIG_FILE_PATH)\r\ncfg = cfgr(MODULE, FUNCTION)\r\nVALUE = cfg[PARAMETER]\r\n```\r\n\r\nThe parameters under the function `init` means that they are defined in module scope, while the parameters under the function `common` means that they are shared among all the functions inside the corresponding module.\r\n\r\n### Locate the Pre-Generated Dataset\r\n\r\nAfter cloning the repository, you can download some pre-generated datasets [here](https://data.mendeley.com/datasets/s9m6tzcv9d) . The datasets described below are organized as [csc sparse matrices](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html), stored in compressed `npz` files using the [function](http://docs.scipy.org/doc/numpy/reference/generated/numpy.savez_compressed.html) of `numpy`. \r\n\r\nFilename | Description  \r\n--- | ---  \r\norig_X.npz | Standard dataset\r\nexp_X.npz | Expanded dataset\r\nudt_orig_X.npz | Standard dataset filtered by UDT\r\nudt_exp_X.npz | Expanded dataset filtered by UDT\r\ndt_orig_X.npz | Standard dataset filtered by DT\r\ndt_exp_X.npz | Expanded dataset filtered by DT\r\nunion_filt_X.npz | Standard dataset filtered by DF\r\nX_[0-9] | Separated standard dataset\r\nY.npz | Cancer hallmark labels\r\ny_[0-9].npz | Separated cancer hallmark label\r\n\r\n**In order to locate the dataset you want to use, please rename it to 'X.npz', and change the parameter `DATA_PATH` of module `bionlp.spider.hoc` inside `etc/config.yaml` into the location of 'X.npz'.**\r\n\r\nYou can load a dataset into a [Pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html), with the corresponding `PMID` as index and each feature as column name, using the utility function `read_df` in `bionlp.util.io`:\r\n\r\n```python\r\nimport bionlp.util.io as io\r\nX = io.read_df('X.npz', with_idx=True, sparse_fmt='csc')\r\n```\r\n\r\n### A Simple Example\r\n\r\nYou can run a demo using the following command:\r\n\r\n```bash\r\nchm_annot.py -m demo\r\n```\r\n\r\nThis demo will download a dataset automatically and perform a 5-fold cross validation on the proposed method UDT-RF. The log is printed to standard output and the results are saved on the disk.\r\n\r\n## Parameter Tuning\r\n\r\nFor the sake of the best performance, you should tune the parameters of your selected model and write them on the model configuration file so that you can use these tuned parameters for model evaluation. \r\n\r\n### Setup parameter range\r\n\r\nYou can edit the function `gen_mdl_params` inside `bin/chm_annot.py` to change the range of parameter tuning. Please uncomment the code lines corresponded to your selected model and change the range of the parameters or append other values you want to test.\r\n\r\n### Run parameter tuning script\r\n\r\nYou can choose an approach for parameter tuning using the following command.\r\n\r\n*Grid Search*:\r\n\r\n```bash\r\nchm_annot.py -t\r\n```\r\n\r\n*Random Search*:\r\n\r\n```bash\r\nchm_annot.py -t -r\r\n```\r\n\r\n### Covert the result to configuration file\r\n\r\nYou can use the utility function in `bin/chm_helper.py` to transformat your tuning result by the following command:\r\n\r\n```bash\r\nchm_helper.py -m n2y -l TUNING_OUTPUT_FOLDER_PATH\r\n```\r\n\r\n**Then copy the basename of the configuration file ended with `.yaml` to the parameter `mdl_cfg` of module `chm_annot` inside `etc/config.yaml`.**\r\n\r\nThe pre-tuned parameters for some models are stored in `etc/mdlcfg.yaml`.\r\n\r\n## Model Evaluation\r\n\r\nYou can use different combination of the feature selection model and classification model to generate a pipeline as the final computational model.\r\n\r\nYou can uncomment the corresponding code lines of the models you want to evaluate in function `gen_featfilt` and `gen_clfs` inside `bin/chm_annot.py` for feature selection and classification respectively.\r\n\r\nIn addition, you can use command line parameter `-c` to adopt the pre-combined model in function `gen_cb_models`. To make use of the parameters stored in configuration file, you can use command line parameter `-c -b` to adopt the pre-combined model with optimized parameters.\r\n\r\n## Dataset Re-Generation\r\n\r\nYou can re-generate the dataset from the [pre-processed files](http://www.cl.cam.ac.uk/~sb895/HoC.html) stored in `DATA_PATH` using the following command:\r\n\r\n```bash\r\nchm_gendata.py -m gen\r\n```\r\n\r\nIt will also generate separated label data `y_[0-9].npz` for single label running.\r\n\r\nFeature selection method can also be applied to the dataset in advance by uncommenting the corresponding code line in function `gen_data` inside `bin\\chm_gendata.py`.\r\n\r\nIf you only want to apply feature selection on the generated dataset or generate separated label data, you can use command line parameter `-l`. Make sure your dataset has already been renamed as 'X.npz' and the processed dataset will be generated as 'new_X.npz'.\r\n\r\n## Common Parameter Setting\r\n\r\n* _-p [0-9]_  \r\nspecify which label you want to use independently\r\n* _-l_  \r\nindicate that you want to use all labels simultaneously\r\n* _-k NUM_  \r\nspecify *K*-fold cross validation\r\n* _-a [micro | macro]_  \r\nspecify which average strategy you want to use for multi-label annotation\r\n* _-n NUM_  \r\nspecify how many CPU cores you want to use simultaneously for parallel computing\r\n\r\n**Other parameter specification can be obtained using `-h`.**",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}